{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# with open(\"TwitterKeys.dat\", \"rb\") as SaveFile:\n",
    "#     ReadData = pickle.load(file=SaveFile)\n",
    "# from twython import Twython\n",
    "# TwitterStream = Twython(ReadData[\"ConsumerKey\"],ReadData[\"ConsumerSecret\"])\n",
    "\n",
    "# Temporarily load tweet data from saved file to keep debug and testing consistent\n",
    "with open(\"TwitterRawData.dat\", \"rb\") as filePath:\n",
    "    TwitterData = pd.DataFrame(pickle.load(file=filePath))[[\"id\", \"full_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1220957331149557765</td>\n",
       "      <td>i guess its time to switch majors. data scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1220955374867701761</td>\n",
       "      <td>#TechnoCool: Data Science Community Rocked by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1220954168057389056</td>\n",
       "      <td>Confused about how data science and data analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1220953376189366272</td>\n",
       "      <td>Creating Robust Python Workflows: Learn to dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1220952323167440896</td>\n",
       "      <td>National Level Seminar on COMPUTATIONAL MATHEM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                          full_text\n",
       "0  1220957331149557765  i guess its time to switch majors. data scienc...\n",
       "1  1220955374867701761  #TechnoCool: Data Science Community Rocked by ...\n",
       "2  1220954168057389056  Confused about how data science and data analy...\n",
       "3  1220953376189366272  Creating Robust Python Workflows: Learn to dev...\n",
       "4  1220952323167440896  National Level Seminar on COMPUTATIONAL MATHEM..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TwitterData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1094 entries, 0 to 1093\n",
      "Data columns (total 2 columns):\n",
      "id           1094 non-null int64\n",
      "full_text    1094 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 17.2+ KB\n"
     ]
    }
   ],
   "source": [
    "TwitterData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1094.000000\n",
       "mean      178.421389\n",
       "std        78.420762\n",
       "min        23.000000\n",
       "25%       111.250000\n",
       "50%       168.000000\n",
       "75%       255.000000\n",
       "max       319.000000\n",
       "Name: full_text, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TwitterData[\"full_text\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    i guess its time to switch majors. data scienc...\n",
      "1    #TechnoCool: Data Science Community Rocked by ...\n",
      "2    Confused about how data science and data analy...\n",
      "3    Creating Robust Python Workflows: Learn to dev...\n",
      "4    National Level Seminar on COMPUTATIONAL MATHEM...\n",
      "Name: full_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "# Convert html encoded special characters to usable format\n",
    "TwitterData[\"full_text\"] = TwitterData[\"full_text\"].apply(lambda x: html.unescape(x))\n",
    "\n",
    "print(TwitterData[\"full_text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    i guess its time to switch majors. data scienc...\n",
      "1    #TechnoCool: Data Science Community Rocked by ...\n",
      "2    Confused about how data science and data analy...\n",
      "3    Creating Robust Python Workflows: Learn to dev...\n",
      "4    National Level Seminar on COMPUTATIONAL MATHEM...\n",
      "Name: full_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Drop URI's completely.\n",
    "TwitterData[\"full_text\"] = TwitterData[\"full_text\"].apply(lambda x: re.sub(string = x, pattern = \"https\\:\\/\\/[\\w]+[.]?[\\w]+?[\\/\\w]+\\/*\", repl = \"\"))\n",
    "\n",
    "print(TwitterData[\"full_text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                   []\n",
      "1    [#TechnoCool, #tech, #technology, #datascience...\n",
      "2    [#CareerKarma, #breakintotech, #21DayCkChallenge]\n",
      "3                                       [#DataScience]\n",
      "4                                                   []\n",
      "Name: HashTags, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Extract hash tags to split later into constituent words\n",
    "TwitterData[\"HashTags\"] = TwitterData[\"full_text\"].apply(lambda x: re.findall(string = x, pattern = r\"\\#\\w+\\b\"))\n",
    "    \n",
    "print(TwitterData[\"HashTags\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    i guess its time to switch majors. data scienc...\n",
      "1    TechnoCool: Data Science Community Rocked by P...\n",
      "2    Confused about how data science and data analy...\n",
      "3    Creating Robust Python Workflows: Learn to dev...\n",
      "4    National Level Seminar on COMPUTATIONAL MATHEM...\n",
      "Name: full_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Converts hashtags to plain words for later processing.\n",
    "TwitterData[\"full_text\"] = TwitterData[\"full_text\"].apply(lambda x: re.sub(string = x, pattern = \"[\\#]*\", repl = \"\"))\n",
    "\n",
    "print(TwitterData[\"full_text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id                                          full_text  \\\n",
      "0  1220957331149557765  i guess its time to switch majors. data scienc...   \n",
      "1  1220955374867701761  TechnoCool: Data Science Community Rocked by P...   \n",
      "2  1220954168057389056  Confused about how data science and data analy...   \n",
      "3  1220953376189366272  Creating Robust Python Workflows: Learn to dev...   \n",
      "4  1220952323167440896  National Level Seminar on COMPUTATIONAL MATHEM...   \n",
      "\n",
      "                                            HashTags  \n",
      "0                                                 []  \n",
      "1  [#TechnoCool, #tech, #technology, #datascience...  \n",
      "2  [#CareerKarma, #breakintotech, #21DayCkChallenge]  \n",
      "3                                     [#DataScience]  \n",
      "4                                                 []  \n"
     ]
    }
   ],
   "source": [
    "# Remove emails and @user\n",
    "TwitterData[\"full_text\"] = TwitterData[\"full_text\"].apply(lambda x: re.sub(string = x, pattern = \"\\b?[a-zA-Z0-9\\.\\_\\%\\+\\-]*@[a-zA-Z0-9\\.\\-\\_]+\\b?\", repl = \"\"))\n",
    "print(TwitterData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1094 entries, 0 to 1093\n",
      "Data columns (total 3 columns):\n",
      "id           1094 non-null int64\n",
      "full_text    1094 non-null object\n",
      "HashTags     1094 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 25.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(TwitterData.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all text to lowercase to simply processing\n",
    "TwitterData[\"full_text\"] = TwitterData[\"full_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and normalize contractions and abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('tagsets')\n",
    "#nltk.download(\"wordnet\")\n",
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct mispellings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary save data to file for debug consistency\n",
    "import pickle\n",
    "with open(\"TwitterTextData.dat\", \"wb\") as filePath:\n",
    "    pickle.dump(TwitterData, file=filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>HashTags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1220957331149557765</td>\n",
       "      <td>i guess its time to switch majors. data scienc...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1220955374867701761</td>\n",
       "      <td>technocool: data science community rocked by p...</td>\n",
       "      <td>[#TechnoCool, #tech, #technology, #datascience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1220954168057389056</td>\n",
       "      <td>confused about how data science and data analy...</td>\n",
       "      <td>[#CareerKarma, #breakintotech, #21DayCkChallenge]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1220953376189366272</td>\n",
       "      <td>creating robust python workflows: learn to dev...</td>\n",
       "      <td>[#DataScience]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1220952323167440896</td>\n",
       "      <td>national level seminar on computational mathem...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                          full_text  \\\n",
       "0  1220957331149557765  i guess its time to switch majors. data scienc...   \n",
       "1  1220955374867701761  technocool: data science community rocked by p...   \n",
       "2  1220954168057389056  confused about how data science and data analy...   \n",
       "3  1220953376189366272  creating robust python workflows: learn to dev...   \n",
       "4  1220952323167440896  national level seminar on computational mathem...   \n",
       "\n",
       "                                            HashTags  \n",
       "0                                                 []  \n",
       "1  [#TechnoCool, #tech, #technology, #datascience...  \n",
       "2  [#CareerKarma, #breakintotech, #21DayCkChallenge]  \n",
       "3                                     [#DataScience]  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open(\"TwitterTextData.dat\", \"rb\") as filePath:\n",
    "    TwitterData = pd.DataFrame(pickle.load(filePath))\n",
    "\n",
    "TwitterData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1094 entries, 0 to 1093\n",
      "Data columns (total 3 columns):\n",
      "id           1094 non-null int64\n",
      "full_text    1094 non-null object\n",
      "HashTags     1094 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 25.8+ KB\n"
     ]
    }
   ],
   "source": [
    "TwitterData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Reference Sentiment\n",
    ">Using TextBlob built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --trusted-host pypi.python.org textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "\n",
    "SentimentData = pd.concat([\n",
    "    TwitterData[\"id\"],\n",
    "    pd.DataFrame(\n",
    "        columns = [\"TextBlobPolarity\", \"TextBlobSentiment\"],\n",
    "        data = [TextBlob(x).sentiment for x in TwitterData[\"full_text\"]],\n",
    "    )\n",
    "], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1094 entries, 0 to 1093\n",
      "Data columns (total 3 columns):\n",
      "id                   1094 non-null int64\n",
      "TextBlobPolarity     1094 non-null float64\n",
      "TextBlobSentiment    1094 non-null float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 25.8 KB\n"
     ]
    }
   ],
   "source": [
    "SentimentData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>TextBlobPolarity</th>\n",
       "      <th>TextBlobSentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.094000e+03</td>\n",
       "      <td>1094.000000</td>\n",
       "      <td>1094.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.220764e+18</td>\n",
       "      <td>0.145139</td>\n",
       "      <td>0.311635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.065543e+13</td>\n",
       "      <td>0.248656</td>\n",
       "      <td>0.291836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.220597e+18</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.220699e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.220758e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.220825e+18</td>\n",
       "      <td>0.260691</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.220957e+18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  TextBlobPolarity  TextBlobSentiment\n",
       "count  1.094000e+03       1094.000000        1094.000000\n",
       "mean   1.220764e+18          0.145139           0.311635\n",
       "std    9.065543e+13          0.248656           0.291836\n",
       "min    1.220597e+18         -0.750000           0.000000\n",
       "25%    1.220699e+18          0.000000           0.000000\n",
       "50%    1.220758e+18          0.000000           0.300000\n",
       "75%    1.220825e+18          0.260691           0.500000\n",
       "max    1.220957e+18          1.000000           1.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>TextBlobPolarity</th>\n",
       "      <th>TextBlobSentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1220957331149557765</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1220955374867701761</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1220954168057389056</td>\n",
       "      <td>0.05625</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1220953376189366272</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1220952323167440896</td>\n",
       "      <td>-0.60000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  TextBlobPolarity  TextBlobSentiment\n",
       "0  1220957331149557765           0.00000               0.00\n",
       "1  1220955374867701761           0.50000               0.50\n",
       "2  1220954168057389056           0.05625               0.55\n",
       "3  1220953376189366272           0.00000               0.00\n",
       "4  1220952323167440896          -0.60000               1.00"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary save data to file for debug consistency\n",
    "import pickle\n",
    "with open(\"TextblobSentiment.dat\", \"wb\") as filePath:\n",
    "    pickle.dump(SentimentData, file=filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>TextBlobPolarity</th>\n",
       "      <th>TextBlobSentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1220957331149557765</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1220955374867701761</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1220954168057389056</td>\n",
       "      <td>0.05625</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1220953376189366272</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1220952323167440896</td>\n",
       "      <td>-0.60000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  TextBlobPolarity  TextBlobSentiment\n",
       "0  1220957331149557765           0.00000               0.00\n",
       "1  1220955374867701761           0.50000               0.50\n",
       "2  1220954168057389056           0.05625               0.55\n",
       "3  1220953376189366272           0.00000               0.00\n",
       "4  1220952323167440896          -0.60000               1.00"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open(\"TextblobSentiment.dat\", \"rb\") as filePath:\n",
    "    SentimentData = pd.DataFrame(pickle.load(filePath))\n",
    "\n",
    "SentimentData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1094 entries, 0 to 1093\n",
      "Data columns (total 3 columns):\n",
      "id                   1094 non-null int64\n",
      "TextBlobPolarity     1094 non-null float64\n",
      "TextBlobSentiment    1094 non-null float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 25.8 KB\n"
     ]
    }
   ],
   "source": [
    "SentimentData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1094.000000\n",
       "mean        0.145139\n",
       "std         0.248656\n",
       "min        -0.750000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.260691\n",
       "max         1.000000\n",
       "Name: TextBlobPolarity, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentData[\"TextBlobPolarity\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sentiment data by training with pre-labeled text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate word counts for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Sentiment Training Dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "TrainingData = pd.read_csv(\"YelpReviewData.csv\", dtype = {\"StarRating\": \"int8\", \"ReviewText\":\"str\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 189354 entries, 0 to 189353\n",
      "Data columns (total 2 columns):\n",
      "StarRating    189354 non-null int8\n",
      "ReviewText    189354 non-null object\n",
      "dtypes: int8(1), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "TrainingData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    43983\n",
       "4    43983\n",
       "1    43983\n",
       "3    33173\n",
       "2    24232\n",
       "Name: StarRating, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingData[\"StarRating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand contractions and abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize words and tag parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop undesirable words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize adjectives, words, nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus.reader import wordnet\n",
    "from nltk.util import ngrams\n",
    "\n",
    "LemmatizerEngine = WordNetLemmatizer()\n",
    "\n",
    "POSTagToLemmaTag_Dict = {\n",
    "    \"J\" : wordnet.ADJ,\n",
    "    \"N\" : wordnet.NOUN,\n",
    "    \"V\" : wordnet.VERB,\n",
    "    \"R\" : wordnet.ADV,\n",
    "}\n",
    "\n",
    "def FilterForKeyWords(TextString):\n",
    "    removeWords_List = list(set([\n",
    "        # Prepositions\n",
    "        \"of\", \"with\", \"without\", \"at\", \"from\", \"into\", \"during\", \"including\", \"until\", \"against\", \"through\", \"throughput\",\n",
    "        \"towards\", \"to\", \"upon\", \"concerning\", \"in\", \"out\", \"for\", \"on\", \"below\", \"by\", \"over\", \"under\", \"despite\",\n",
    "        \"before\", \"after\", \"between\", \"since\", \"among\", \"along\", \"following\", \"across\", \"behind\", \"beyond\", \"except\",\n",
    "        \"but\", \"up\", \"down\", \"aboard\", \"amid\", \"as\", \"behind\", \"considering\", \"during\", \"inside\", \"minus\", \"off\", \"per\",\n",
    "        \"versus\", \"via\",\n",
    "    ]))\n",
    "    alphaCheck = re.compile(r\"^[a-z]+$\")\n",
    "\n",
    "    return str([LemmatizerEngine.lemmatize(word, POSTagToLemmaTag_Dict[pos[0]]) for (word, pos) in nltk.pos_tag(nltk.word_tokenize(TextString.lower()))\n",
    "            if (\n",
    "                (len(word) > 1)\n",
    "                & (alphaCheck.match(word) != None)\n",
    "                & (word not in removeWords_List)\n",
    "                & (pos[0] in [\n",
    "                    \"J\",#\"JJ\", \"JJR\", \"JJS\", # Adjectives\n",
    "                    #\"N\",#\"NN\", \"NNS\", \"NNP\", \"NNPS\", # Nouns\n",
    "                    \"R\",#\"RB\", \"RBR\", \"RBS\", # Adverbs\n",
    "                    \"V\",#\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", #Verbs\n",
    "                    ])\n",
    "               )\n",
    "               ])\n",
    "\n",
    "def GetPrincipalWordCounts(DataFrame, TextColumnName):\n",
    "    from datetime import datetime # For debugging performance data\n",
    "\n",
    "    Vectorizer = CountVectorizer(lowercase = False, strip_accents = \"ascii\", preprocessor = FilterForKeyWords,\n",
    "                                 min_df = 0.005, ngram_range = (1, 3),\n",
    "                                )\n",
    "    startTime = datetime.now() # For debugging performance data\n",
    "\n",
    "    # Filter out unwanted words in each row, then create count columns for remaining words \n",
    "    WordCounts = pd.DataFrame(\n",
    "        Vectorizer.fit_transform(DataFrame[TextColumnName]).toarray(), \n",
    "        columns=Vectorizer.get_feature_names(), \n",
    "        dtype = \"uint8\",\n",
    "    )\n",
    "\n",
    "    print(str(datetime.now() - startTime)) # For debugging performance data\n",
    "\n",
    "    print(WordCounts.info())\n",
    "    return WordCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs.'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingData[\"ReviewText\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing Start.  2020-01-30 04:17:43.561429\n",
      "0:00:09.423795\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Columns: 1999 entries, able to yummy be\n",
      "dtypes: uint8(1999)\n",
      "memory usage: 1.9 MB\n",
      "None\n",
      "Vectorizing End.  2020-01-30 04:17:53.152775\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(\"Vectorizing Start. \", str(datetime.now()))\n",
    "TrainingDataWordCounts = GetPrincipalWordCounts(TrainingData.iloc[:1000, :], \"ReviewText\")\n",
    "print(\"Vectorizing End. \", str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>able get</th>\n",
       "      <th>about</th>\n",
       "      <th>about be</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accommodate</th>\n",
       "      <th>accompany</th>\n",
       "      <th>accompany be</th>\n",
       "      <th>acknowledge</th>\n",
       "      <th>act</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yell</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>yet be</th>\n",
       "      <th>yet have</th>\n",
       "      <th>young</th>\n",
       "      <th>young be</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yummy be</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1999 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  able get  about  about be  absolutely  accommodate  accompany  \\\n",
       "0     0         0      0         0           0            0          0   \n",
       "1     0         0      0         0           0            0          0   \n",
       "2     0         0      0         0           0            0          0   \n",
       "3     0         0      0         0           0            0          0   \n",
       "4     0         0      0         0           0            0          0   \n",
       "\n",
       "   accompany be  acknowledge  act  ...  yeah  yell  yes  yet  yet be  \\\n",
       "0             0            0    0  ...     0     0    0    0       0   \n",
       "1             0            0    0  ...     0     0    0    0       0   \n",
       "2             0            1    0  ...     0     0    0    1       0   \n",
       "3             0            0    0  ...     0     0    0    0       0   \n",
       "4             0            0    0  ...     0     0    0    0       0   \n",
       "\n",
       "   yet have  young  young be  yummy  yummy be  \n",
       "0         0      0         0      0         0  \n",
       "1         0      0         0      0         0  \n",
       "2         0      0         0      0         0  \n",
       "3         0      0         0      0         0  \n",
       "4         0      0         0      0         0  \n",
       "\n",
       "[5 rows x 1999 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingDataWordCounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "be highly          5\n",
       "very little        5\n",
       "end have           5\n",
       "have only be       5\n",
       "be great too       5\n",
       "                ... \n",
       "get              728\n",
       "not              879\n",
       "do              1041\n",
       "have            1757\n",
       "be              6229\n",
       "Length: 1999, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingDataWordCounts.sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Z:\\\\Downloads\\\\yelp_dataset\\\\yelp_dataset~\\\\TrainingDataWordCounts.dat\", \"wb\") as filePath:\n",
    "    pickle.dump(TrainingDataWordCounts, file=filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Z:\\\\Downloads\\\\yelp_dataset\\\\yelp_dataset~\\\\TrainingDataWordCounts.dat\", \"rb\") as filePath:\n",
    "    TrainingDataWordCounts = pickle.load(file=filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 189354 entries, 0 to 189353\n",
      "Columns: 1743 entries, 00 to zero\n",
      "dtypes: uint8(1743)\n",
      "memory usage: 314.8 MB\n"
     ]
    }
   ],
   "source": [
    "TrainingDataWordCounts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate matching word count columns from Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.105048\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1094 entries, 0 to 1093\n",
      "Columns: 514 entries, 10 to your\n",
      "dtypes: uint8(514)\n",
      "memory usage: 549.3 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "TwitterDataWordCounts = GetPrincipalWordCounts(TwitterData, \"full_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1483"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(TrainingDataWordCounts.columns) - set(TwitterDataWordCounts.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(TwitterDataWordCounts.columns) - set(TrainingDataWordCounts.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1094 entries, 0 to 1093\n",
      "Columns: 1743 entries, 00 to zero\n",
      "dtypes: int64(1483), uint8(260)\n",
      "memory usage: 12.6 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1094 entries, 0 to 1093\n",
      "Columns: 1743 entries, 00 to zero\n",
      "dtypes: int8(1743)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Remove columns for words not in model\n",
    "\n",
    "# Setup resulting Dataframe to ensure word columns align.\n",
    "SentimentDataWordCounts = pd.DataFrame(columns = TrainingDataWordCounts.columns)\n",
    "\n",
    "# Copy over matching columns with data\n",
    "for column in TwitterWordCounts.columns.to_list():\n",
    "    if(column in SentimentDataWordCounts.columns.to_list()):\n",
    "        SentimentDataWordCounts[column] = TwitterWordCounts[column]\n",
    "\n",
    "# Fill missing word columns with 0\n",
    "SentimentDataWordCounts = SentimentDataWordCounts.fillna(0)\n",
    "\n",
    "for column in SentimentDataWordCounts.columns.to_list():\n",
    "    SentimentDataWordCounts[column] = SentimentDataWordCounts[column].astype(\"int8\")\n",
    "\n",
    "SentimentDataWordCounts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    43983\n",
       "4    43983\n",
       "1    43983\n",
       "3    33173\n",
       "2    24232\n",
       "Name: StarRating, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingData[\"StarRating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingData[\"StarRating\"] = TrainingData[\"StarRating\"].map({1:0.0, 2:0.4, 3:0.8, 4:1.2, 5:2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingData = pd.concat([TrainingData[\"StarRating\"], WordCounts], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 189354 entries, 0 to 189353\n",
      "Columns: 1744 entries, StarRating to zero\n",
      "dtypes: float64(1), uint8(1743)\n",
      "memory usage: 316.2 MB\n"
     ]
    }
   ],
   "source": [
    "TrainingData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(TrainingData.drop(\"StarRating\", axis = 1), TrainingData[\"StarRating\"], test_size = 0.2, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151483, 1743)\n",
      "(151483,)\n",
      "(37871, 1743)\n",
      "(37871,)\n"
     ]
    }
   ],
   "source": [
    "print(Train_X.shape)\n",
    "print(Train_Y.shape)\n",
    "print(Test_X.shape)\n",
    "print(Test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Z:\\\\Downloads\\\\yelp_dataset\\\\yelp_dataset~\\\\TrainTestData.dat\", \"wb\") as filePath:\n",
    "    pickle.dump(Train_X, file=filePath)\n",
    "    pickle.dump(Train_Y, file=filePath)\n",
    "    pickle.dump(Test_X, file=filePath)\n",
    "    pickle.dump(Test_Y, file=filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Z:\\\\Downloads\\\\yelp_dataset\\\\yelp_dataset~\\\\TrainTestData.dat\", \"rb\") as filePath:\n",
    "    Train_X = pickle.load(file=filePath)\n",
    "    Train_Y = pickle.load(file=filePath)\n",
    "    Test_X = pickle.load(file=filePath)\n",
    "    Test_Y = pickle.load(file=filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151483, 1743)\n",
      "(151483,)\n",
      "(37871, 1743)\n",
      "(37871,)\n"
     ]
    }
   ],
   "source": [
    "print(Train_X.shape)\n",
    "print(Train_Y.shape)\n",
    "print(Test_X.shape)\n",
    "print(Test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through multiple classifiers and rank results\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB, ComplementNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "def AssessClassifierModels(TrainingDataColumns, TrainingDataResults, TestingDataColumns, TestingDataResults, Algorithms_List):\n",
    "    from datetime import datetime\n",
    "    functionStartTime = datetime.now()\n",
    "    print()\n",
    "    \n",
    "    # results container\n",
    "    results_list = pd.DataFrame( columns = [\"Name\",\n",
    "                                            \"Precision\",\n",
    "                                            \"Recall\",\n",
    "                                            \"F1\",\n",
    "                                            \"Support\",\n",
    "                                            \"ModelData\",\n",
    "                                            \"ExecutionTime\",\n",
    "                                           ]) # Set index later to avoid empty row\n",
    "    \n",
    "    # calculated metrics and append to list\n",
    "    for algorithm in Algorithms_List:\n",
    "        loopStartTime = datetime.now()\n",
    "        print(\"Starting \" + str(algorithm.__name__) + \" at \" + str(loopStartTime))\n",
    "\n",
    "        algorithmObject = algorithm()\n",
    "        \n",
    "        if(str(algorithm.__name__) == \"XGBClassifier\"):\n",
    "            algorithmObject = XGBClassifier(nthread=4)\n",
    "\n",
    "        algorithmObject.fit(TrainingDataColumns, TrainingDataResults)\n",
    "        algorithmPredictions = algorithmObject.predict(TestingDataColumns)\n",
    "        (algorithmPrecision, algorithmRecall, algorithmF1, algorithmSupportList) = precision_recall_fscore_support(\n",
    "            TestingDataResults, algorithmPredictions, labels = np.sort(TrainingDataResults.unique()))\n",
    "        algorithmExecutionTime = str(datetime.now() - loopStartTime)\n",
    "        \n",
    "        results_list = results_list.append({\"Name\":  algorithm.__name__,\n",
    "                                            \"Precision\": algorithmPrecision,\n",
    "                                            \"Recall\": algorithmRecall,\n",
    "                                            \"F1\": algorithmF1,\n",
    "                                            \"Support\": algorithmSupportList,\n",
    "#                                            \"ConfusionMatrix\": \"\",# confusion_matrix(TestingDataResults, algorithmPredictions),\n",
    "                                            \"ModelData\" : algorithmObject,\n",
    "                                            \"ExecutionTime\": algorithmExecutionTime, \n",
    "                                            }, ignore_index = True)\n",
    "#         print(\"\\tEnding \" + str(algorithm.__name__) + \" at \" + str(datetime.now()) + \"\\n\")\n",
    "        \n",
    "#         with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None, \"display.max_colwidth\", 1000):\n",
    "#             print(results_list.iloc[len(results_list)-1, :])\n",
    "\n",
    "    # Set index to a meaningful value\n",
    "    results_list.set_index(\"Name\")\n",
    "    print(\"Assessment Complete.\")\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ClassifierResults_List = AssessClassifierModels(Train_X, Train_Y.apply(str).astype(\"category\"), Test_X, Test_Y.apply(str).astype(\"category\"), [XGBClassifier, MultinomialNB, GaussianNB, BernoulliNB, KNeighborsClassifier, DecisionTreeClassifier, ExtraTreeClassifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None, \"display.max_colwidth\", 1000):\n",
    "    print(ClassifierResults_List[[\"Name\", \"Precision\", \"Recall\", \"F1\", \"Support\", \"ExecutionTime\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to file for debug consistency\n",
    "import pickle\n",
    "with open(\"Z:\\\\Downloads\\\\yelp_dataset\\\\yelp_dataset~\\\\ClassifierResults.dat\", \"wb\") as filePath:\n",
    "    pickle.dump(ClassifierResults_List, file=filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "#del ClassifierResults_List\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Name  \\\n",
      "0           XGBClassifier   \n",
      "1           MultinomialNB   \n",
      "2              GaussianNB   \n",
      "3             BernoulliNB   \n",
      "4  DecisionTreeClassifier   \n",
      "5     ExtraTreeClassifier   \n",
      "\n",
      "                                                                                                 Precision  \\\n",
      "0    [0.43410852713178294, 0.62510845045983, 0.46622542339887096, 0.45076060848678945, 0.5988313520048358]   \n",
      "1    [0.32357043235704325, 0.6955237446034992, 0.4434848484848485, 0.5111710323574731, 0.6925350122060902]   \n",
      "2    [0.2851963746223565, 0.6786147419485244, 0.3448133635801196, 0.39832775919732444, 0.4130954570535222]   \n",
      "3  [0.30156537753222834, 0.6761139311900733, 0.37977315689981095, 0.45304172027200884, 0.3993250127356088]   \n",
      "4    [0.21397849462365592, 0.5853964632059326, 0.2966114572253457, 0.3647869815798395, 0.5114114779525815]   \n",
      "5    [0.19717376904393905, 0.4862628268785171, 0.2645865834633385, 0.3355742935278031, 0.4458374573848015]   \n",
      "\n",
      "                                                                                                   Recall  \\\n",
      "0   [0.11925042589437819, 0.8214570744498917, 0.35933983495873967, 0.506636670416198, 0.6717143180020342]   \n",
      "1   [0.29642248722316866, 0.6979819860905256, 0.4391597899474869, 0.5970753655793026, 0.6091083738275511]   \n",
      "2   [0.2010221465076661, 0.5741648614753164, 0.2508627156789197, 0.26794150731158606, 0.7614419708441632]   \n",
      "3  [0.27896081771720616, 0.5466879489225858, 0.3014253563390848, 0.27727784026996627, 0.7086676460617019]   \n",
      "4  [0.2118824531516184, 0.5849960095770151, 0.29287321830457613, 0.36310461192350957, 0.5216408633743926]   \n",
      "5    [0.1901618398637138, 0.5024512598335423, 0.254463615903976, 0.3312710911136108, 0.45813086224432137]   \n",
      "\n",
      "                                                                                                         F1  \\\n",
      "0    [0.18710324089542266, 0.7099571365226388, 0.4058634129808507, 0.47706810719203474, 0.6331824234354195]   \n",
      "1        [0.30940208935318964, 0.69675069709213, 0.4413117225782133, 0.5507938155027498, 0.648148148148148]   \n",
      "2      [0.2358231326505121, 0.6220355731225296, 0.2904290429042904, 0.3203765971755212, 0.5356120826709062]   \n",
      "3    [0.28982300884955753, 0.6045514719788185, 0.3360936846507737, 0.34400948991696323, 0.5108133425650634]   \n",
      "4    [0.21292531564305586, 0.5851961678832117, 0.2947304846746187, 0.36394385252832745, 0.5164755244755245]   \n",
      "5  [0.19360433604336044, 0.49422451497140296, 0.25942638623326963, 0.33340880787954263, 0.4519005684984952]   \n",
      "\n",
      "                          Support   ExecutionTime  \n",
      "0  [4696, 8771, 6665, 8890, 8849]  0:08:59.657520  \n",
      "1  [4696, 8771, 6665, 8890, 8849]  0:00:02.971040  \n",
      "2  [4696, 8771, 6665, 8890, 8849]  0:00:24.687082  \n",
      "3  [4696, 8771, 6665, 8890, 8849]  0:00:49.803198  \n",
      "4  [4696, 8771, 6665, 8890, 8849]  0:03:24.986719  \n",
      "5  [4696, 8771, 6665, 8890, 8849]  0:00:11.191309  \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"Z:\\\\Downloads\\\\yelp_dataset\\\\yelp_dataset~\\\\ClassifierResults.dat\", \"rb\") as filePath:\n",
    "    ClassifierResults_List = pickle.load(file=filePath)\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None, \"display.max_colwidth\", 1000):\n",
    "        print(ClassifierResults_List.drop(\"ModelData\", axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictionModel = ClassifierResults_List.loc[ClassifierResults_List[\"Name\"] == \"MultinomialNB\", \"ModelData\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del ClassifierResults_List\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Twitter Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompareClassificationPredictions(TestData, ComparisonData):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "    (precision, recall, f1score, supportList) = precision_recall_fscore_support(TestData, ComparisonData, labels = np.sort(TestData.unique()))\n",
    "    metrics = pd.DataFrame(data = {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1score,\n",
    "        \"Support\": supportList,\n",
    "        })\n",
    "\n",
    "\n",
    "#     metrics = pd.DataFrame(data = {\n",
    "#         \"Accuracy\": accuracy_score(TestData, ComparisonData),\n",
    "#         \"Precision\": precision_score(TestData, ComparisonData),\n",
    "#         \"Recall\": recall_score(TestData, ComparisonData),\n",
    "#         \"F1\": [f1_score(TestData, ComparisonData)],\n",
    "#         })\n",
    "    \n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None, \"display.max_colwidth\", 1000):\n",
    "        print(\"Confusion Matrix\")\n",
    "        print(confusion_matrix(TestData, ComparisonData))\n",
    "        print(\"\")\n",
    "        print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SentimentPredictions = pd.Series(PredictionModel.predict(TwitterAdjustedWordCounts)).astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare predictions to standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1094.000000\n",
       "mean        0.102377\n",
       "std         0.922442\n",
       "min        -1.000000\n",
       "25%        -1.000000\n",
       "50%         0.500000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentPredictions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1094.000000\n",
       "mean        0.145139\n",
       "std         0.248656\n",
       "min        -0.750000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.260691\n",
       "max         1.000000\n",
       "Name: TextBlobPolarity, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentData[\"TextBlobPolarity\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[  7   0   6]\n",
      " [372  11 524]\n",
      " [ 59   8 107]]\n",
      "\n",
      "   Precision    Recall        F1  Support\n",
      "0   0.015982  0.538462  0.031042       13\n",
      "1   0.578947  0.012128  0.023758      907\n",
      "2   0.167975  0.614943  0.263872      174\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CompareClassificationPredictions(SentimentData[\"TextBlobPolarity\"].apply(lambda x: 1.0 if (x > 0.4) else -1.0 if (x < -0.4) else 0.0), SentimentPredictions.apply(lambda x: 1.0 if (x > 0.4) else -1.0 if (x < -0.4) else 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def AssessXGBRegression(x_train, y_train, x_test, y_test, MaxDepth = 6, LearningRate = 0.1):\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBRegressor\n",
    "    from datetime import datetime\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "    loopStartTime = datetime.now()\n",
    "    # results container\n",
    "    results_list = pd.DataFrame( columns = [\"Name\", \"R2 Score\", \"RMS Error\", \"Mean Absolute Error\", \"ModelData\", \"ExecutionTime\"])\n",
    "\n",
    "    xgbModel = XGBRegressor(objective = \"reg:squarederror\",\n",
    "                            colsample_bytree = 1,\n",
    "                            colsample_bylevel = 1,\n",
    "                            colsample_bynode = 1,\n",
    "                            learning_rate = LearningRate,\n",
    "                            max_depth = MaxDepth,\n",
    "                            tree_method = \"hist\",\n",
    "                            grow_policy = \"lossguide\",\n",
    "                            n_estimators = 200,\n",
    "                            nthread = 6,\n",
    "                           )\n",
    "\n",
    "    print(\"Starting XGBRegressor at \" + str(loopStartTime))\n",
    "    print(\"\\tLearning Rate: \", str(LearningRate), \"\\tTree Depth: \", str(MaxDepth))\n",
    "\n",
    "    xgbModel.fit(x_train, y_train)\n",
    "    xgbPredictor = xgbModel.predict(x_test)\n",
    "\n",
    "    loopEndTime = datetime.now()\n",
    "    results_list = results_list.append({\"Name\" : \"XGBRegressor\",\n",
    "                                        \"R2 Score\": r2_score(y_test, xgbPredictor),\n",
    "                                        \"RMS Error\": (mean_squared_error(y_test, xgbPredictor)),\n",
    "                                        \"Mean Absolute Error\": mean_absolute_error(y_test, xgbPredictor),\n",
    "                                        \"ModelData\" : xgbModel,\n",
    "                                        \"ExecutionTime\": str(loopEndTime - loopStartTime),\n",
    "                                        \"LearningRate\": str(LearningRate),\n",
    "                                        \"MaxDepth\":str(MaxDepth),\n",
    "                                        }, ignore_index = True)\n",
    "\n",
    "    print(\"\\tEnding XGBRegressor at \" + str(datetime.now()))\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompareRegressionPredictions(TestData, ComparisonData):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, confusion_matrix\n",
    "    \n",
    "    metrics = pd.DataFrame(data = {\n",
    "        \"R2 Score\": r2_score(TestData, ComparisonData),\n",
    "        \"RMS Error\": mean_squared_error(TestData, ComparisonData),\n",
    "        \"Mean Absolute Error\": [mean_absolute_error(TestData, ComparisonData)],\n",
    "        })\n",
    "    \n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None, \"display.max_colwidth\", 1000):\n",
    "        print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBRegressor at 2020-01-25 15:40:34.170720\n",
      "\tLearning Rate:  0.2 \tTree Depth:  9\n",
      "\tEnding XGBRegressor at 2020-01-25 15:45:25.172392\n",
      "           Name  R2 Score  RMS Error  Mean Absolute Error   ExecutionTime  \\\n",
      "0  XGBRegressor  0.642646   0.186775             0.338399  0:04:50.985665   \n",
      "\n",
      "  LearningRate MaxDepth  \n",
      "0          0.2        9  \n"
     ]
    }
   ],
   "source": [
    "XGBResults = AssessXGBRegression(Train_X, Train_Y, Test_X, Test_Y, LearningRate = 0.2, MaxDepth = 9)\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None, \"display.max_colwidth\", 1000):\n",
    "    print(XGBResults.drop(\"ModelData\", axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"XGBRegressorResults.dat\", \"rb\") as filePath:\n",
    "    XGBResults = pickle.load(file=filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>RMS Error</th>\n",
       "      <th>Mean Absolute Error</th>\n",
       "      <th>ModelData</th>\n",
       "      <th>ExecutionTime</th>\n",
       "      <th>LearningRate</th>\n",
       "      <th>MaxDepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>0.667182</td>\n",
       "      <td>0.734399</td>\n",
       "      <td>0.671007</td>\n",
       "      <td>XGBRegressor(base_score=0.5, booster='gbtree',...</td>\n",
       "      <td>0:04:37.756705</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name  R2 Score  RMS Error  Mean Absolute Error  \\\n",
       "0  XGBRegressor  0.667182   0.734399             0.671007   \n",
       "\n",
       "                                           ModelData   ExecutionTime  \\\n",
       "0  XGBRegressor(base_score=0.5, booster='gbtree',...  0:04:37.756705   \n",
       "\n",
       "  LearningRate MaxDepth  \n",
       "0          0.2        9  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictionModel = XGBResults[\"ModelData\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SentimentPredictions = pd.Series(PredictionModel.predict(TwitterAdjustedWordCounts)).astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R2 Score  RMS Error  Mean Absolute Error\n",
      "0  -0.56532   0.096695              0.23428\n"
     ]
    }
   ],
   "source": [
    "CompareRegressionPredictions(SentimentData[\"TextBlobPolarity\"].apply(lambda x: (x + 1.0)), SentimentPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1094.000000\n",
       "mean        0.944958\n",
       "std         0.151800\n",
       "min         0.403817\n",
       "25%         0.892358\n",
       "50%         0.911873\n",
       "75%         0.963613\n",
       "max         1.612821\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentPredictions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
