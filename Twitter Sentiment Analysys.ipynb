{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# with open(\"TwitterKeys.txt\", \"rb\") as SaveFile:\n",
    "#     ReadData = pickle.load(file=SaveFile)\n",
    "# from twython import Twython\n",
    "# TwitterStream = Twython(ReadData[\"ConsumerKey\"],ReadData[\"ConsumerSecret\"])\n",
    "# TwitterData = TwitterStream.search(q=\"data science -filter:retweets AND -filter:replies\", count=20, tweet_mode = \"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily load tweet data from saved file to keep debug and testing consistent\n",
    "with open(\"TwitterRawData.txt\", \"rb\") as SaveFile:\n",
    "    TwitterData = pd.DataFrame(pickle.load(file=SaveFile)[\"statuses\"])[[\"id\", \"full_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1219667475710849024</td>\n",
       "      <td>Congratulations to Ashwani Dev, Director, Digi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1219667319322042372</td>\n",
       "      <td>We believe Earth systems science is a key to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1219667296416944129</td>\n",
       "      <td>A empresa de data science entra em contato dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1219667248404750336</td>\n",
       "      <td>Genomics are a hot topic in the science and me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1219667136727191552</td>\n",
       "      <td>We're #recuriting for the role of 'Director of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                          full_text\n",
       "0  1219667475710849024  Congratulations to Ashwani Dev, Director, Digi...\n",
       "1  1219667319322042372  We believe Earth systems science is a key to c...\n",
       "2  1219667296416944129  A empresa de data science entra em contato dep...\n",
       "3  1219667248404750336  Genomics are a hot topic in the science and me...\n",
       "4  1219667136727191552  We're #recuriting for the role of 'Director of..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TwitterData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 2 columns):\n",
      "id           20 non-null int64\n",
      "full_text    20 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 448.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "TwitterData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     20.000000\n",
       "mean     166.550000\n",
       "std       76.239564\n",
       "min       76.000000\n",
       "25%      100.750000\n",
       "50%      142.000000\n",
       "75%      234.750000\n",
       "max      304.000000\n",
       "Name: full_text, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TwitterData[\"full_text\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     testData = TwitterData.copy()\n",
    "#     testData[0] = testData[0] + \" something@email.com\"\n",
    "#     HashTags = list()\n",
    "#     for rowIndex in range(len(testData)):\n",
    "#         print(\"========================\\n\")\n",
    "#         print(str.format(\"Tweet {0}:\\n\", str(rowIndex)))\n",
    "#         print(\"-----------------------------------------------\\n\")\n",
    "#         print(testData[rowIndex], \"\\n\")\n",
    "#         print(\"-----------------------------------------------\\n\")\n",
    "#         #print(re.sub(string = testData[rowIndex], pattern = \"\\b?[a-zA-Z0-9\\.\\_\\%\\+\\-]*@[a-zA-Z0-9\\.\\-\\_]+\\b?\", repl = \"\"), \"\\n\")\n",
    "#         hashTags = re.findall(string = testData[rowIndex], pattern = r\"\\#\\w+\\b\")\n",
    "#         if hashTags:\n",
    "#             print(\"Found #: \", hashTags)\n",
    "#             HashTags.append(hashTags)\n",
    "#         else:\n",
    "#             HashTags.append(list())\n",
    "#         print(\"===============================================\\n\")\n",
    "#     print(HashTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testData = pd.DataFrame()\n",
    "# testData[\"full_text\"] = TwitterData.copy()\n",
    "# testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     New Closter show announcement! Data Science No...\n",
      "1     Data science career advice from Shaun Wheeler ...\n",
      "2     Nice review on #infections in #hemodialysis pa...\n",
      "3     The Bay Area Air Quality Management District a...\n",
      "4     \"The archaeological evidence from Moscerini su...\n",
      "5     𝐅𝐄𝐀𝐓𝐔𝐑𝐄𝐃 𝐂𝐎𝐔𝐑𝐒𝐄𝐒\\n\\nThe #Data #Science #Course...\n",
      "6     data science with python online training, Plot...\n",
      "7     Galaxy Brain: Data Science Workflows with Pref...\n",
      "8     Practical Ways to Integrate Data Science Into ...\n",
      "9     More Ethical and Empowering Data Science | MIT...\n",
      "10    Level up your data science vocabulary: Linearl...\n",
      "11    “Smarter Pricing for Airbnb Using Machine Lear...\n",
      "12    Methods of Study Design – Experiments https://...\n",
      "13    Our Data Science training will guide you step ...\n",
      "14    On 1/22 in #Chicago, learn from Kyle Chard, Re...\n",
      "15    Data Science on R By OrangeTree Global https:/...\n",
      "16    We've changed the #algorithm https://t.co/DT9s...\n",
      "17    Which Data Science Skills are core and which a...\n",
      "18    Password and hijacked email dataset for you to...\n",
      "19    Going to the Big Bang Data Science & Analytics...\n",
      "Name: full_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "# Convert html encoded special characters to usable format\n",
    "TwitterData[\"full_text\"] = TwitterData[\"full_text\"].apply(lambda x: html.unescape(x))\n",
    "\n",
    "print(TwitterData[\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     New Closter show announcement! Data Science No...\n",
      "1     Data science career advice from Shaun Wheeler ...\n",
      "2     Nice review on #infections in #hemodialysis pa...\n",
      "3     The Bay Area Air Quality Management District a...\n",
      "4     \"The archaeological evidence from Moscerini su...\n",
      "5     𝐅𝐄𝐀𝐓𝐔𝐑𝐄𝐃 𝐂𝐎𝐔𝐑𝐒𝐄𝐒\\n\\nThe #Data #Science #Course...\n",
      "6     data science with python online training, Plot...\n",
      "7     Galaxy Brain: Data Science Workflows with Pref...\n",
      "8     Practical Ways to Integrate Data Science Into ...\n",
      "9     More Ethical and Empowering Data Science | MIT...\n",
      "10    Level up your data science vocabulary: Linearl...\n",
      "11    “Smarter Pricing for Airbnb Using Machine Lear...\n",
      "12              Methods of Study Design – Experiments  \n",
      "13    Our Data Science training will guide you step ...\n",
      "14    On 1/22 in #Chicago, learn from Kyle Chard, Re...\n",
      "15    Data Science on R By OrangeTree Global   #mach...\n",
      "16                       We've changed the #algorithm  \n",
      "17    Which Data Science Skills are core and which a...\n",
      "18    Password and hijacked email dataset for you to...\n",
      "19    Going to the Big Bang Data Science & Analytics...\n",
      "Name: full_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Drop URI's completely.\n",
    "TwitterData[\"full_text\"] = TwitterData[\"full_text\"].apply(lambda x: re.sub(string = x, pattern = \"https\\:\\/\\/[\\w]+[.]?[\\w]+?[\\/\\w]+\\/*\", repl = \"\"))\n",
    "\n",
    "print(TwitterData[\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                    []\n",
      "1                                                    []\n",
      "2     [#infections, #hemodialysis, #grafts, #IDTwitt...\n",
      "3                                                    []\n",
      "4                                          [#oceanhist]\n",
      "5     [#Data, #Science, #Course, #Best, #Seller, #Tr...\n",
      "6                                        [#bharatpages]\n",
      "7                                        [#DataScience]\n",
      "8                                        [#DataScience]\n",
      "9                                        [#DataScience]\n",
      "10               [#Vector, #LinearlyIndependentVectors]\n",
      "11                                                   []\n",
      "12                                                   []\n",
      "13        [#datascience, #stayrelevantforever, #python]\n",
      "14                                           [#Chicago]\n",
      "15                              [#machinelearning, #ad]\n",
      "16                                         [#algorithm]\n",
      "17    [#ArtificialIntelligence, #MachineLearning, #D...\n",
      "18                                                   []\n",
      "19                                                   []\n",
      "Name: HashTags, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Extract hash tags to split later into constituent words\n",
    "TwitterData[\"HashTags\"] = TwitterData[\"full_text\"].apply(lambda x: re.findall(string = x, pattern = r\"\\#\\w+\\b\"))\n",
    "    \n",
    "print(TwitterData[\"HashTags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     New Closter show announcement! Data Science No...\n",
      "1     Data science career advice from Shaun Wheeler ...\n",
      "2     Nice review on infections in hemodialysis pati...\n",
      "3     The Bay Area Air Quality Management District a...\n",
      "4     \"The archaeological evidence from Moscerini su...\n",
      "5     𝐅𝐄𝐀𝐓𝐔𝐑𝐄𝐃 𝐂𝐎𝐔𝐑𝐒𝐄𝐒\\n\\nThe Data Science Course 20...\n",
      "6     data science with python online training, Plot...\n",
      "7     Galaxy Brain: Data Science Workflows with Pref...\n",
      "8     Practical Ways to Integrate Data Science Into ...\n",
      "9     More Ethical and Empowering Data Science | MIT...\n",
      "10    Level up your data science vocabulary: Linearl...\n",
      "11    “Smarter Pricing for Airbnb Using Machine Lear...\n",
      "12              Methods of Study Design – Experiments  \n",
      "13    Our Data Science training will guide you step ...\n",
      "14    On 1/22 in Chicago, learn from Kyle Chard, Res...\n",
      "15    Data Science on R By OrangeTree Global   machi...\n",
      "16                        We've changed the algorithm  \n",
      "17    Which Data Science Skills are core and which a...\n",
      "18    Password and hijacked email dataset for you to...\n",
      "19    Going to the Big Bang Data Science & Analytics...\n",
      "Name: full_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Converts hashtags to plain words for later processing.\n",
    "TwitterData[\"full_text\"] = TwitterData[\"full_text\"].apply(lambda x: re.sub(string = x, pattern = \"[\\#]*\", repl = \"\"))\n",
    "\n",
    "print(TwitterData[\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id                                          full_text  \\\n",
      "0   1217601537716957184  New Closter show announcement! Data Science No...   \n",
      "1   1217601407831937026  Data science career advice from Shaun Wheeler ...   \n",
      "2   1217601129028050944  Nice review on infections in hemodialysis pati...   \n",
      "3   1217601125022674944  The Bay Area Air Quality Management District a...   \n",
      "4   1217599748141670401  \"The archaeological evidence from Moscerini su...   \n",
      "5   1217599556621422592  𝐅𝐄𝐀𝐓𝐔𝐑𝐄𝐃 𝐂𝐎𝐔𝐑𝐒𝐄𝐒\\n\\nThe Data Science Course 20...   \n",
      "6   1217599535385628672  data science with python online training, Plot...   \n",
      "7   1217599399221694464  Galaxy Brain: Data Science Workflows with Pref...   \n",
      "8   1217599398173081600  Practical Ways to Integrate Data Science Into ...   \n",
      "9   1217599396910596097  More Ethical and Empowering Data Science | MIT...   \n",
      "10  1217599126759563264  Level up your data science vocabulary: Linearl...   \n",
      "11  1217598565608054787  “Smarter Pricing for Airbnb Using Machine Lear...   \n",
      "12  1217598398335016961            Methods of Study Design – Experiments     \n",
      "13  1217597856363745280  Our Data Science training will guide you step ...   \n",
      "14  1217597380117377026  On 1/22 in Chicago, learn from Kyle Chard, Res...   \n",
      "15  1217597371359670272  Data Science on R By OrangeTree Global   machi...   \n",
      "16  1217597345954770947                      We've changed the algorithm     \n",
      "17  1217597086797127680  Which Data Science Skills are core and which a...   \n",
      "18  1217597083529678848  Password and hijacked email dataset for you to...   \n",
      "19  1217596588375400450  Going to the Big Bang Data Science & Analytics...   \n",
      "\n",
      "                                             HashTags  \n",
      "0                                                  []  \n",
      "1                                                  []  \n",
      "2   [#infections, #hemodialysis, #grafts, #IDTwitt...  \n",
      "3                                                  []  \n",
      "4                                        [#oceanhist]  \n",
      "5   [#Data, #Science, #Course, #Best, #Seller, #Tr...  \n",
      "6                                      [#bharatpages]  \n",
      "7                                      [#DataScience]  \n",
      "8                                      [#DataScience]  \n",
      "9                                      [#DataScience]  \n",
      "10             [#Vector, #LinearlyIndependentVectors]  \n",
      "11                                                 []  \n",
      "12                                                 []  \n",
      "13      [#datascience, #stayrelevantforever, #python]  \n",
      "14                                         [#Chicago]  \n",
      "15                            [#machinelearning, #ad]  \n",
      "16                                       [#algorithm]  \n",
      "17  [#ArtificialIntelligence, #MachineLearning, #D...  \n",
      "18                                                 []  \n",
      "19                                                 []  \n"
     ]
    }
   ],
   "source": [
    "# Remove emails and @user\n",
    "TwitterData[\"full_text\"] = TwitterData[\"full_text\"].apply(lambda x: re.sub(string = x, pattern = \"\\b?[a-zA-Z0-9\\.\\_\\%\\+\\-]*@[a-zA-Z0-9\\.\\-\\_]+\\b?\", repl = \"\"))\n",
    "print(TwitterData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 3 columns):\n",
      "id           20 non-null int64\n",
      "full_text    20 non-null object\n",
      "HashTags     20 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 608.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Separate compound words (e.g. Hashtags as sentences)\n",
    "\n",
    "\n",
    "#print(testData[\"full_text\"])\n",
    "\n",
    "# #testData[\"HashTags\"] = HashTags\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     for rowIndex in range(len(testData)):\n",
    "#         print(testData[rowIndex], \"\\n\")\n",
    "#         print(\"-----------\")\n",
    "\n",
    "# testData = pd.concat([testData, pd.Series(HashTags).rename(\"HashTags\")], axis = 1)\n",
    "\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     for rowIndex in range(len(testData)):\n",
    "#         print(testData[rowIndex], \"\\n\")\n",
    "#         print(\"-----------\")\n",
    "print(TwitterData.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('tagsets')\n",
    "#nltk.download(\"wordnet\")\n",
    "#nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all text to lowercase to simply processing\n",
    "TwitterData[\"full_text\"] = TwitterData[\"full_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and normalize contractions and abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct mispellings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"TwitterTextData.txt\", \"wb\") as SaveFile:\n",
    "    pickle.dump(TwitterData, file=SaveFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Sentiment with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --trusted-host pypi.python.org textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>HashTags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1217601537716957184</td>\n",
       "      <td>new closter show announcement! data science no...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1217601407831937026</td>\n",
       "      <td>data science career advice from shaun wheeler ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1217601129028050944</td>\n",
       "      <td>nice review on infections in hemodialysis pati...</td>\n",
       "      <td>[#infections, #hemodialysis, #grafts, #IDTwitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1217601125022674944</td>\n",
       "      <td>the bay area air quality management district a...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1217599748141670401</td>\n",
       "      <td>\"the archaeological evidence from moscerini su...</td>\n",
       "      <td>[#oceanhist]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                          full_text  \\\n",
       "0  1217601537716957184  new closter show announcement! data science no...   \n",
       "1  1217601407831937026  data science career advice from shaun wheeler ...   \n",
       "2  1217601129028050944  nice review on infections in hemodialysis pati...   \n",
       "3  1217601125022674944  the bay area air quality management district a...   \n",
       "4  1217599748141670401  \"the archaeological evidence from moscerini su...   \n",
       "\n",
       "                                            HashTags  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2  [#infections, #hemodialysis, #grafts, #IDTwitt...  \n",
       "3                                                 []  \n",
       "4                                       [#oceanhist]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open(\"TwitterTextData.txt\", \"rb\") as SaveFile:\n",
    "    TwitterData = pd.DataFrame(pickle.load(SaveFile))\n",
    "\n",
    "TwitterData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1217601537716957184</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1217601407831937026</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1217601129028050944</td>\n",
       "      <td>-0.043750</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1217601125022674944</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1217599748141670401</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1217599556621422592</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1217599535385628672</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1217599399221694464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1217599398173081600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1217599396910596097</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1217599126759563264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1217598565608054787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1217598398335016961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1217597856363745280</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.477273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1217597380117377026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1217597371359670272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1217597345954770947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1217597086797127680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1217597083529678848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1217596588375400450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  polarity  subjectivity\n",
       "0   1217601537716957184  0.102273      0.318182\n",
       "1   1217601407831937026  0.100000      0.150000\n",
       "2   1217601129028050944 -0.043750      0.531250\n",
       "3   1217601125022674944  0.500000      0.500000\n",
       "4   1217599748141670401  0.050000      0.150000\n",
       "5   1217599556621422592  0.320000      0.420000\n",
       "6   1217599535385628672  0.166667      0.333333\n",
       "7   1217599399221694464  0.000000      0.000000\n",
       "8   1217599398173081600  0.000000      0.000000\n",
       "9   1217599396910596097  0.233333      0.366667\n",
       "10  1217599126759563264  0.000000      0.125000\n",
       "11  1217598565608054787  0.000000      0.000000\n",
       "12  1217598398335016961  0.000000      0.000000\n",
       "13  1217597856363745280  0.318182      0.477273\n",
       "14  1217597380117377026  0.000000      0.000000\n",
       "15  1217597371359670272  0.000000      0.000000\n",
       "16  1217597345954770947  0.000000      0.000000\n",
       "17  1217597086797127680  0.000000      0.000000\n",
       "18  1217597083529678848  0.000000      0.000000\n",
       "19  1217596588375400450  0.000000      0.100000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "SentimentData = pd.concat([\n",
    "    TwitterData[\"id\"],\n",
    "    pd.DataFrame(\n",
    "        columns = [\"TextBlobPolarity\", \"TextBlobSentiment\"],\n",
    "        data = [TextBlob(x).sentiment for x in TwitterData[\"full_text\"]],\n",
    "    )\n",
    "], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 3 columns):\n",
      "id                   20 non-null int64\n",
      "TextBlobPolarity     20 non-null float64\n",
      "TextBlobSentiment    20 non-null float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 608.0 bytes\n"
     ]
    }
   ],
   "source": [
    "SentimentData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>TextBlobPolarity</th>\n",
       "      <th>TextBlobSentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1217601537716957184</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1217601407831937026</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1217601129028050944</td>\n",
       "      <td>-0.043750</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1217601125022674944</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1217599748141670401</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  TextBlobPolarity  TextBlobSentiment\n",
       "0  1217601537716957184          0.102273           0.318182\n",
       "1  1217601407831937026          0.100000           0.150000\n",
       "2  1217601129028050944         -0.043750           0.531250\n",
       "3  1217601125022674944          0.500000           0.500000\n",
       "4  1217599748141670401          0.050000           0.150000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import RegexpStemmer\n",
    "#stemmer = nltk.stem.RegexpStemmer(\"ing\")\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "Lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenLemmaMapping = pd.DataFrame({\n",
    "    \"Token\" : [\"v\"],\n",
    "    \"Lemma\" : [\"n\"],\n",
    "})\n",
    "print(TwitterData[\"full_text\"][0])\n",
    "print(Lemmatizer.lemmatize(TwitterData[\"full_text\"][0].split()[24], pos = \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenLemmaMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenLemmaMapping[\"v\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and tag parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(TwitterData[\"full_text\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate average polarity of adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
